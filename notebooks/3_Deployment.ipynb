{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.modeling.impute import SimpleImputer\n",
    "from snowflake.ml.modeling.metrics import accuracy_score\n",
    "from snowflake.ml.modeling.model_selection import GridSearchCV\n",
    "from snowflake.ml.modeling.preprocessing import OneHotEncoder\n",
    "from snowflake.ml.modeling.xgboost import XGBClassifier\n",
    "from snowflake.ml.registry import Registry\n",
    "from snowflake.ml.utils.connection_params import SnowflakeLoginOptions\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark import types as T\n",
    "from snowflake.snowpark.functions import col\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session.builder.configs(SnowflakeLoginOptions()).getOrCreate()\n",
    "\n",
    "model_version = yaml.safe_load(open(\"../params.yaml\"))[\"model\"][\"version\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = session.table(\"titanic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"SURVIVED\"  |\"PCLASS\"  |\"AGE\"  |\"SIBSP\"  |\"PARCH\"  |\"FARE\"   |\"ADULT_MALE\"  |\"DECK\"  |\"ALIVE\"  |\"ALONE\"  |\"SEX\"   |\"EMBARKED\"  |\"CLASS\"  |\"WHO\"  |\"EMBARK_TOWN\"  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|0           |3         |22.00  |1        |0        |7.2500   |True          |NULL    |False    |False    |MALE    |S           |THIRD    |MAN    |SOUTHAMPTON    |\n",
      "|1           |1         |38.00  |1        |0        |71.2833  |False         |C       |True     |False    |FEMALE  |C           |FIRST    |WOMAN  |CHERBOURG      |\n",
      "|1           |3         |26.00  |0        |0        |7.9250   |False         |NULL    |True     |True     |FEMALE  |S           |THIRD    |WOMAN  |SOUTHAMPTON    |\n",
      "|1           |1         |35.00  |1        |0        |53.1000  |False         |C       |True     |False    |FEMALE  |S           |FIRST    |WOMAN  |SOUTHAMPTON    |\n",
      "|0           |3         |35.00  |0        |0        |8.0500   |True          |NULL    |False    |True     |MALE    |S           |THIRD    |MAN    |SOUTHAMPTON    |\n",
      "|0           |3         |NULL   |0        |0        |8.4583   |True          |NULL    |False    |True     |MALE    |Q           |THIRD    |MAN    |QUEENSTOWN     |\n",
      "|0           |1         |54.00  |0        |0        |51.8625  |True          |E       |False    |True     |MALE    |S           |FIRST    |MAN    |SOUTHAMPTON    |\n",
      "|0           |3         |2.00   |3        |1        |21.0750  |False         |NULL    |False    |False    |MALE    |S           |THIRD    |CHILD  |SOUTHAMPTON    |\n",
      "|1           |3         |27.00  |0        |2        |11.1333  |False         |NULL    |True     |False    |FEMALE  |S           |THIRD    |WOMAN  |SOUTHAMPTON    |\n",
      "|1           |2         |14.00  |1        |0        |30.0708  |False         |NULL    |True     |False    |FEMALE  |C           |SECOND   |CHILD  |CHERBOURG      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AGE': 177, 'DECK': 688, 'EMBARKED': 2, 'EMBARK_TOWN': 2}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns with null values and their respective counts\n",
    "{\n",
    "    column_name: count_of_null\n",
    "    for column_name, count_of_null in {\n",
    "        col_name: titanic_df.where(col(col_name).is_null()).count()\n",
    "        for col_name in titanic_df.columns\n",
    "    }.items()\n",
    "    if count_of_null > 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.drop(\n",
    "    [\"AGE\", \"DECK\", \"ALIVE\", \"ADULT_MALE\", \"EMBARKED\", \"SEX\", \"PCLASS\", \"ALONE\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "|\"SURVIVED\"  |\"SIBSP\"  |\"PARCH\"  |\"CLASS\"  |\"WHO\"  |\"EMBARK_TOWN\"  |\"FARE\"   |\n",
      "------------------------------------------------------------------------------\n",
      "|0           |1        |0        |THIRD    |MAN    |SOUTHAMPTON    |7.25     |\n",
      "|1           |1        |0        |FIRST    |WOMAN  |CHERBOURG      |71.2833  |\n",
      "|1           |0        |0        |THIRD    |WOMAN  |SOUTHAMPTON    |7.925    |\n",
      "|1           |1        |0        |FIRST    |WOMAN  |SOUTHAMPTON    |53.1     |\n",
      "|0           |0        |0        |THIRD    |MAN    |SOUTHAMPTON    |8.05     |\n",
      "|0           |0        |0        |THIRD    |MAN    |QUEENSTOWN     |8.4583   |\n",
      "|0           |0        |0        |FIRST    |MAN    |SOUTHAMPTON    |51.8625  |\n",
      "|0           |3        |1        |THIRD    |CHILD  |SOUTHAMPTON    |21.075   |\n",
      "|1           |0        |2        |THIRD    |WOMAN  |SOUTHAMPTON    |11.1333  |\n",
      "|1           |1        |0        |SECOND   |CHILD  |CHERBOURG      |30.0708  |\n",
      "------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df = titanic_df.withColumn(\"FARE\", titanic_df[\"FARE\"].astype(T.FloatType()))\n",
    "\n",
    "titanic_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\"CLASS\", \"WHO\", \"EMBARK_TOWN\"]\n",
    "num_cols = [\"SIBSP\", \"PARCH\", \"FARE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "|\"CLASS\"  |\"WHO\"  |\"EMBARK_TOWN\"  |\"SURVIVED\"  |\"SIBSP\"  |\"PARCH\"  |\"FARE\"   |\n",
      "------------------------------------------------------------------------------\n",
      "|THIRD    |MAN    |SOUTHAMPTON    |0           |1        |0        |7.25     |\n",
      "|FIRST    |WOMAN  |CHERBOURG      |1           |1        |0        |71.2833  |\n",
      "|THIRD    |WOMAN  |SOUTHAMPTON    |1           |0        |0        |7.925    |\n",
      "|FIRST    |WOMAN  |SOUTHAMPTON    |1           |1        |0        |53.1     |\n",
      "|THIRD    |MAN    |SOUTHAMPTON    |0           |0        |0        |8.05     |\n",
      "|THIRD    |MAN    |QUEENSTOWN     |0           |0        |0        |8.4583   |\n",
      "|FIRST    |MAN    |SOUTHAMPTON    |0           |0        |0        |51.8625  |\n",
      "|THIRD    |CHILD  |SOUTHAMPTON    |0           |3        |1        |21.075   |\n",
      "|THIRD    |WOMAN  |SOUTHAMPTON    |1           |0        |2        |11.1333  |\n",
      "|SECOND   |CHILD  |CHERBOURG      |1           |1        |0        |30.0708  |\n",
      "------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "impute_cat = SimpleImputer(\n",
    "    input_cols=cat_cols,\n",
    "    output_cols=cat_cols,\n",
    "    strategy=\"most_frequent\",\n",
    "    drop_input_cols=True,\n",
    ")\n",
    "\n",
    "titanic_df = impute_cat.fit(titanic_df).transform(titanic_df)\n",
    "titanic_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/imputer.pkl', 'wb') as file: \n",
    "      \n",
    "    # A new file will be created \n",
    "    pickle.dump(impute_cat, file) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/snowflake_ml_py311/lib/python3.11/site-packages/snowflake/ml/modeling/preprocessing/one_hot_encoder.py:853: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  state_pandas[[_CATEGORY]] = state_pandas[[_CATEGORY]].applymap(convert_to_string_excluding_nan)\n",
      "/opt/anaconda3/envs/snowflake_ml_py311/lib/python3.11/site-packages/snowflake/ml/modeling/preprocessing/one_hot_encoder.py:854: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  state_pandas[[_FITTED_CATEGORY]] = state_pandas[[_FITTED_CATEGORY]].applymap(convert_to_string_excluding_nan)\n",
      "/opt/anaconda3/envs/snowflake_ml_py311/lib/python3.11/site-packages/snowflake/ml/modeling/preprocessing/one_hot_encoder.py:853: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  state_pandas[[_CATEGORY]] = state_pandas[[_CATEGORY]].applymap(convert_to_string_excluding_nan)\n",
      "/opt/anaconda3/envs/snowflake_ml_py311/lib/python3.11/site-packages/snowflake/ml/modeling/preprocessing/one_hot_encoder.py:854: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  state_pandas[[_FITTED_CATEGORY]] = state_pandas[[_FITTED_CATEGORY]].applymap(convert_to_string_excluding_nan)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"CLASS_SECOND\"  |\"CLASS_THIRD\"  |\"WHO_MAN\"  |\"WHO_WOMAN\"  |\"EMBARK_TOWN_QUEENSTOWN\"  |\"EMBARK_TOWN_SOUTHAMPTON\"  |\"SURVIVED\"  |\"SIBSP\"  |\"PARCH\"  |\"FARE\"   |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|0.0             |1.0            |1.0        |0.0          |0.0                       |1.0                        |0           |1        |0        |7.25     |\n",
      "|0.0             |0.0            |0.0        |1.0          |0.0                       |0.0                        |1           |1        |0        |71.2833  |\n",
      "|0.0             |1.0            |0.0        |1.0          |0.0                       |1.0                        |1           |0        |0        |7.925    |\n",
      "|0.0             |0.0            |0.0        |1.0          |0.0                       |1.0                        |1           |1        |0        |53.1     |\n",
      "|0.0             |1.0            |1.0        |0.0          |0.0                       |1.0                        |0           |0        |0        |8.05     |\n",
      "|0.0             |1.0            |1.0        |0.0          |1.0                       |0.0                        |0           |0        |0        |8.4583   |\n",
      "|0.0             |0.0            |1.0        |0.0          |0.0                       |1.0                        |0           |0        |0        |51.8625  |\n",
      "|0.0             |1.0            |0.0        |0.0          |0.0                       |1.0                        |0           |3        |1        |21.075   |\n",
      "|0.0             |1.0            |0.0        |1.0          |0.0                       |1.0                        |1           |0        |2        |11.1333  |\n",
      "|1.0             |0.0            |0.0        |0.0          |0.0                       |0.0                        |1           |1        |0        |30.0708  |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "OHE = OneHotEncoder(\n",
    "    input_cols=cat_cols,\n",
    "    output_cols=cat_cols,\n",
    "    drop_input_cols=True,\n",
    "    drop=\"first\",\n",
    "    handle_unknown=\"ignore\",\n",
    ")\n",
    "\n",
    "titanic_df = OHE.fit(titanic_df).transform(titanic_df)\n",
    "titanic_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/onehotencoder.pkl', 'wb') as file: \n",
    "      \n",
    "    # A new file will be created \n",
    "    pickle.dump(OHE, file) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir)) + '/data'\n",
    "titanic_pd = pd.DataFrame(titanic_df.collect())\n",
    "cleaned_dir = os.makedirs(data_dir + \"/training\", exist_ok=True)\n",
    "titanic_pd.to_csv(\"../data/training/titanic.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = titanic_df.random_split(weights=[0.8, 0.2], seed=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"n_estimators\": [100, 200, 300, 400, 500],\n",
    "    \"learning_rate\": [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    \"max_depth\": list(range(3, 6, 1)),\n",
    "    \"min_child_weight\": list(range(1, 6, 1)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\n",
    "    f\"ALTER WAREHOUSE {session.get_current_warehouse()[1:-1]} SET WAREHOUSE_SIZE=LARGE;\"\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data scientists may not have the ability to change the warehouse size.  They will usually have access to a larger warehouse and can easily switch as well using session.use_warehouse('bigger_warehouse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    estimator=XGBClassifier(),\n",
    "    param_grid=parameters,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"accuracy\",\n",
    "    input_cols=train_df.drop(\"SURVIVED\").columns,\n",
    "    label_cols=\"SURVIVED\",\n",
    "    output_cols=\"PRED_SURVIVED\",\n",
    ")\n",
    "\n",
    "# Train\n",
    "grid_search.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\n",
    "    f\"ALTER WAREHOUSE {session.get_current_warehouse()[1:-1]} SET WAREHOUSE_SIZE=XSMALL;\"\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = grid_search.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(\n",
    "    df=result, y_true_col_names=\"SURVIVED\", y_pred_col_names=\"PRED_SURVIVED\"\n",
    ")\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print each combination of hyperparameters with their accuracy\n",
    "results = grid_search.to_sklearn().cv_results_\n",
    "data = {\"accuracy\": results[\"mean_test_score\"]}\n",
    "for i, param in enumerate(results[\"params\"]):\n",
    "    for key, value in param.items():\n",
    "        if key not in data:\n",
    "            data[key] = [None] * len(results[\"params\"])\n",
    "        data[key][i] = value\n",
    "\n",
    "# Create DataFrame\n",
    "hp_df = pd.DataFrame(data).sort_values(by=\"accuracy\", ascending=False)\n",
    "hp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Registry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_model = grid_search.to_sklearn().best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to add one to our model versions if it already exists\n",
    "\n",
    "\n",
    "def check_and_update(df, model_name):\n",
    "    if df.empty:\n",
    "        return \"V_1\"\n",
    "    elif df[df[\"name\"] == model_name].empty:\n",
    "        return \"V_1\"\n",
    "    else:\n",
    "        # Increment model_version if df is not a pandas Series\n",
    "        lst = sorted(ast.literal_eval(df[\"versions\"][0]))\n",
    "        last_value = lst[-1]\n",
    "        prefix, num = last_value.rsplit(\"_\", 1)\n",
    "        new_last_value = f\"{prefix}_{int(num)+1}\"\n",
    "        lst[-1] = new_last_value\n",
    "        return new_last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Registry Object\n",
    "reg = Registry\n",
    "\n",
    "# Get sample input data to pass into the registry logging function\n",
    "X = train_df.drop(\"SURVIVED\").limit(100)\n",
    "\n",
    "\n",
    "# Define model name and version (use uppercase for name)\n",
    "\n",
    "titanic_model = reg.log_model(\n",
    "    model_name=\"TITANIC\",\n",
    "    version_name=model_version,\n",
    "    model=optimal_model,\n",
    "    sample_input_data=X,\n",
    ")\n",
    "\n",
    "# Add evaluation metric\n",
    "titanic_model.set_metric(\n",
    "    metric_name=\"accuracy\",\n",
    "    value=hp_df[\"accuracy\"][0],\n",
    ")\n",
    "\n",
    "session.file.put(\"../models/imputer.pkl\", \"@ml_data/models_\" + model_version, overwrite=True)\n",
    "session.file.put(\"../models/onehotencoder.pkl\", \"@ml_data/models_\" + model_version, overwrite=True)\n",
    "session.file.put(\"../data/training/titanic.csv\", \"@ml_data/data_\" + model_version + \"/training\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.show_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    k: v for k, v in optimal_model.get_params().items() if v and k != \"missing\"\n",
    "}\n",
    "titanic_model.set_metric(metric_name=\"hyperparameters\", value=hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 500\n",
    "reg.get_model(model_name).show_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have multiple versions of the model, we want the UDF to be deployed as the version with the highest accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_df = reg.get_model(model_name).show_versions()\n",
    "reg_df[\"accuracy\"] = reg_df[\"metadata\"].apply(\n",
    "    lambda x: json.loads(x)[\"metrics\"][\"accuracy\"]\n",
    ")\n",
    "best_model = reg_df.sort_values(by=\"accuracy\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_version = best_model[\"name\"].iloc[0]\n",
    "deployed_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the default version to the deployed version (best model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = reg.get_model(model_name)\n",
    "m.default = deployed_version\n",
    "mv = m.default\n",
    "mv.version_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_prediction = mv.run(test_df, function_name=\"predict_proba\")\n",
    "remote_prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test in SQL write test data back to a table\n",
    "\n",
    "test_df.write.mode(\"overwrite\").save_as_table(\"TEST_DATA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add images to stage for Streamlit App\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.file.put(\"../streamlit_images/*\", \"@ML_DATA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling Model from SQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy this code in a snowflake worksheet or run via session.sql\n",
    "inference_df = session.sql('''\n",
    "select *, TITANIC!predict_proba(*):output_feature_0\n",
    "as surv_pred\n",
    "from (\n",
    "select * exclude survived\n",
    "from test_data)\n",
    "            ''')\n",
    "inference_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calling model from a new notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point to the registry\n",
    "\n",
    "reg = Registry(session=session)\n",
    "\n",
    "# Get the default version of your model (Model with best accuracy in our case)\n",
    "\n",
    "mv = reg.get_model(\"titanic\").default\n",
    "\n",
    "remote_prediction = mv.run(test_df, function_name=\"predict_proba\")\n",
    "remote_prediction.drop('\"output_feature_0\"').with_column_renamed(\n",
    "    '\"output_feature_1\"', \"pred_survived\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To delete your model and all of it's versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reg.delete_model(\"TITANIC\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Intro_SnowML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
